% This file was created with Citavi 6.10.0.0

@misc{Antonelli.2021,
 author = {Antonelli, Michela and Reinke, Annika and Bakas, Spyridon and Farahani, Keyvan and AnnetteKopp-Schneider and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and {van Ginneken}, Bram and Bilello, Michel and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc J. and Heckers, Stephan H. and Huisman, Henkjan and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Pernicka, Jennifer S. Goli and Rhode, Kawal and Tobon-Gomez, Catalina and Vorontsov, Eugene and Meakin, James A. and Ourselin, Sebastien and Wiesenfarth, Manuel and Arbelaez, Pablo and Bae, Byeonguk and Chen, Sihong and Daza, Laura and Feng, Jianjiang and He, Baochun and Isensee, Fabian and Ji, Yuanfeng and Jia, Fucang and Kim, Namkug and Kim, Ildoo and Merhof, Dorit and Pai, Akshay and Park, Beomhee and Perslev, Mathias and Rezaiifar, Ramin and Rippel, Oliver and Sarasua, Ignacio and Shen, Wei and Son, Jaemin and Wachinger, Christian and Wang, Liansheng and Wang, Yan and Xia, Yingda and Xu, Daguang and Xu, Zhanwei and Zheng, Yefeng and Simpson, Amber L. and Maier-Hein, Lena and Cardoso, M. Jorge},
 year = {2021},
 title = {The Medical Segmentation Decathlon},
 keywords = {68T07;Computer Vision;electronic engineering;FOS: Computer;FOS: Electrical engineering;Image;information engineering;information sciences;Machine Learning (cs.LG);Pattern Recognition (cs.CV);Video Processing (eess.IV)},
 abstract = {},
 doi = {10.48550/ARXIV.2106.05735},
 publisher = {arXiv}
}


@misc{Dataloader.2020,
 year = {2020},
 title = {PersistentDataset, CacheDataset, and simple Dataset Tutorial and Speed Test},
 url = {https://github.com/Project-MONAI/tutorials/blob/main/acceleration/dataset_type_performance.ipynb},
 urldate = {2022-06-29},
 howpublished = {Github},
 abstract = {},
 organization = {{Project Monai}}
}


@misc{Isensee.2018,
 author = {Isensee, Fabian and Petersen, Jens and Klein, Andre and Zimmerer, David and Jaeger, Paul F. and Kohl, Simon and Wasserthal, Jakob and Koehler, Gregor and Norajitra, Tobias and Wirkert, Sebastian and Maier-Hein, Klaus H.},
 year = {2018},
 title = {nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation},
 url = {http://arxiv.org/pdf/1809.10486v1},
 abstract = {The U-Net was presented in 2015. With its straight-forward and successful architecture it quickly evolved to a commonly used benchmark in medical image segmentation. The adaptation of the U-Net to novel problems, however, comprises several degrees of freedom regarding the exact architecture, preprocessing, training and inference. These choices are not independent of each other and substantially impact the overall performance. The present paper introduces the nnU-Net ('no-new-Net'), which refers to a robust and self-adapting framework on the basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away superfluous bells and whistles of many proposed network designs and instead focus on the remaining aspects that make out the performance and generalizability of a method. We evaluate the nnU-Net in the context of the Medical Segmentation Decathlon challenge, which measures segmentation performance in ten disciplines comprising distinct entities, image modalities, image geometries and dataset sizes, with no manual adjustments between datasets allowed. At the time of manuscript submission, nnU-Net achieves the highest mean dice scores across all classes and seven phase 1 tasks (except class 1 in BrainTumour) in the online leaderboard of the challenge.}
}


@article{Isensee.2021,
 author = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A. A. and Petersen, Jens and Maier-Hein, Klaus H.},
 year = {2020},
 title = {nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation},
 pages = {203--211},
 pagination = {page},
 volume = {18},
 journaltitle = {Nature methods},
 language = {eng},
 doi = {10.1038/s41592-020-01008-z},
 number = {2},
 abstract = {Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.},
 note = {Journal Article

Research Support, Non-U.S. Gov't},
 eprint = {33288961}
}


@misc{Luth.2022,
 author = {L{\"u}th, Carsten and sten2lu and Bungert, Till},
 year = {2022},
 title = {nnU-Net Workshop},
 url = {https://github.com/IML-DKFZ/nnunet-workshop},
 urldate = {2022-06-29},
 abstract = {},
 language = {en},
 location = {Github},
 version = {c75cdab31d73ab82f12b97b2352b55f4ed246633}
}


@article{Menze.2015,
 author = {Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc-Andr{\'e} and Arbel, Tal and Avants, Brian B. and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J. and Criminisi, Antonio and Das, Tilak and Delingette, Herv{\'e} and Demiralp, {\c{C}}a{\u{g}}atay and Durst, Christopher R. and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M. and Jena, Raj and John, Nigel M. and Konukoglu, Ender and Lashkari, Danial and Mariz, Jos{\'e} Antoni{\'o} and Meier, Raphael and Pereira, S{\'e}rgio and Precup, Doina and Price, Stephen J. and Raviv, Tammy Riklin and Reza, Syed M. S. and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo-Chang and Shotton, Jamie and Silva, Carlos A. and Sousa, Nuno and Subbanna, Nagesh K. and Szekely, Gabor and Taylor, Thomas J. and Thomas, Owen M. and Tustison, Nicholas J. and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and {van Leemput}, Koen},
 year = {2015},
 title = {The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)},
 pages = {1993--2024},
 pagination = {page},
 volume = {34},
 journaltitle = {IEEE transactions on medical imaging},
 language = {eng},
 doi = {10.1109/TMI.2014.2377694},
 number = {10},
 abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients-manually annotated by up to four raters-and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74{\%}-85{\%}), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
 note = {Journal Article

Research Support, N.I.H., Extramural

Research Support, Non-U.S. Gov't

Review},
 eprint = {25494501}
}


@misc{Monai.2020,
 year = {2020},
 title = {Brain tumor 3D segmentation with MONAI},
 url = {https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb},
 urldate = {2022-06-29},
 howpublished = {Github},
 abstract = {},
 version = {418f9db42e36c19d81e4dab984a3830ac57128be},
 organization = {{Project Monai}}
}


@misc{Myronenko.2018,
 author = {Myronenko, Andriy},
 year = {2018},
 title = {3D MRI brain tumor segmentation using autoencoder regularization},
 url = {http://arxiv.org/pdf/1810.11654v3},
 abstract = {Automated segmentation of brain tumors from 3D magnetic resonance images (MRIs) is necessary for the diagnosis, monitoring, and treatment planning of the disease. Manual delineation practices require anatomical knowledge, are expensive, time consuming and can be inaccurate due to human error. Here, we describe a semantic segmentation network for tumor subregion segmentation from 3D MRIs based on encoder-decoder architecture. Due to a limited training dataset size, a variational auto-encoder branch is added to reconstruct the input image itself in order to regularize the shared decoder and impose additional constraints on its layers. The current approach won 1st place in the BraTS 2018 challenge.}
}


